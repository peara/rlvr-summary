# Default model configuration for Llama-3-1B
# This will be used for baseline training in Phase A

_target_: transformers.AutoModelForCausalLM.from_pretrained

model_name: "meta-llama/Llama-3.2-1B"
trust_remote_code: true

# Quantization settings
load_in_8bit: false
load_in_4bit: false
torch_dtype: "auto"

# Tokenizer configuration
tokenizer:
  padding_side: "left"  # Use left padding for causal models
  trust_remote_code: true

# Generation settings
generation:
  max_new_tokens: 256
  temperature: 0.7
  top_p: 0.9
  do_sample: true
  pad_token_id: null  # Will be set from tokenizer

# Model-specific settings
use_cache: true
device_map: "auto"